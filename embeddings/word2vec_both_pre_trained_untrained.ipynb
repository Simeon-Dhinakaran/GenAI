{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simeon-Dhinakaran/GenAI/blob/main/embeddings/word2vec_both_pre_trained_untrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay56lAJKX38J",
        "outputId": "80403579-cc98-4063-c752-417b7480364a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'example':\n",
            "[ 9.7702928e-03  8.1651136e-03  1.2809718e-03  5.0975787e-03\n",
            "  1.4081288e-03 -6.4551616e-03 -1.4280510e-03  6.4491653e-03\n",
            " -4.6173059e-03 -3.9930656e-03  4.9244044e-03  2.7130984e-03\n",
            " -1.8479753e-03 -2.8769434e-03  6.0107317e-03 -5.7167388e-03\n",
            " -3.2367026e-03 -6.4878250e-03 -4.2346325e-03 -8.5809948e-03\n",
            " -4.4697891e-03 -8.5112294e-03  1.4037776e-03 -8.6181965e-03\n",
            " -9.9166557e-03 -8.2016252e-03 -6.7726658e-03  6.6805850e-03\n",
            "  3.7845564e-03  3.5616636e-04 -2.9579818e-03 -7.4283206e-03\n",
            "  5.3341867e-04  4.9989222e-04  1.9561886e-04  8.5259555e-04\n",
            "  7.8633073e-04 -6.8160298e-05 -8.0070542e-03 -5.8702733e-03\n",
            " -8.3829118e-03 -1.3120425e-03  1.8206370e-03  7.4171280e-03\n",
            " -1.9634271e-03 -2.3252917e-03  9.4871549e-03  7.9704521e-05\n",
            " -2.4045217e-03  8.6048469e-03  2.6870037e-03 -5.3439722e-03\n",
            "  6.5881060e-03  4.5101536e-03 -7.0544672e-03 -3.2317400e-04\n",
            "  8.3448651e-04  5.7473574e-03 -1.7176545e-03 -2.8065301e-03\n",
            "  1.7484308e-03  8.4717153e-04  1.1928272e-03 -2.6342822e-03\n",
            " -5.9857843e-03  7.3229838e-03  7.5873756e-03  8.2963575e-03\n",
            " -8.5988473e-03  2.6364254e-03 -3.5599626e-03  9.6204039e-03\n",
            "  2.9037679e-03  4.6411133e-03  2.3856151e-03  6.6084778e-03\n",
            " -5.7432903e-03  7.8944126e-03 -2.4109220e-03 -4.5618857e-03\n",
            " -2.0609903e-03  9.7335577e-03 -6.8565905e-03 -2.1917201e-03\n",
            "  7.0009995e-03 -5.5749417e-05 -6.2949671e-03 -6.3935257e-03\n",
            "  8.9403950e-03  6.4295758e-03  4.7735930e-03 -3.2620477e-03\n",
            " -9.2676198e-03  3.7868882e-03  7.1605504e-03 -5.6328895e-03\n",
            " -7.8650126e-03 -2.9727400e-03 -4.9318983e-03 -2.3151112e-03]\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"this is a small example\",\n",
        "    \"word embeddings are very useful\",\n",
        "    \"we can use tensorflow to create embeddings\"\n",
        "]\n",
        "\n",
        "# Preprocess the corpus\n",
        "processed_corpus = [simple_preprocess(doc) for doc in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Example usage: get the embedding for a word\n",
        "word = 'example'\n",
        "embedding = model.wv[word]\n",
        "\n",
        "print(f\"Embedding for '{word}':\\n{embedding}\")\n",
        "print(len(embedding))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"He is Walter\",\n",
        "    \"He is William\",\n",
        "    \"He isn’t Peter or September\"\n",
        "]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Example: Get the vector for the word 'Walter'\n",
        "vector = model.wv['Walter']\n",
        "print(\"Vector for 'Walter':\", vector)\n",
        "\n",
        "# Example: Find most similar words to 'Walter'\n",
        "similar_words = model.wv.most_similar('Walter')\n",
        "print(\"Words most similar to 'Walter':\", similar_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3reUkPBCWa7",
        "outputId": "27154564-c7de-4d6b-c5a9-62f2ac515665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'Walter': [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
            " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
            " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
            "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
            "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
            "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
            " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
            " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
            " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
            " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
            "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
            "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
            "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
            "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
            " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
            "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
            "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
            " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
            "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
            "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
            " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
            "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
            "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
            " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
            " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n",
            "Words most similar to 'Walter': [('He', 0.09291724115610123), ('William', 0.00484249135479331), ('September', -0.0027540253940969706), ('or', -0.013679751195013523), ('Peter', -0.028491031378507614), ('isn’t', -0.05774581804871559), ('is', -0.11555545777082443)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the text8 corpus\n",
        "dataset = api.load(\"text8\")\n",
        "\n",
        "# Build the Word2Vec model\n",
        "model = Word2Vec(dataset, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model\n",
        "model.save('text8_word2vec.model')\n",
        "\n",
        "# Load the saved model\n",
        "model = Word2Vec.load('text8_word2vec.model')\n",
        "\n",
        "# Example: Get the vector for a word\n",
        "word = \"king\"\n",
        "if word in model.wv.key_to_index:\n",
        "    vector = model.wv[word]\n",
        "    print(f\"Embedding for the word '{word}':\\n{vector}\")\n",
        "else:\n",
        "    print(f\"The word '{word}' is not in the vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_gHot55Ccwb",
        "outputId": "53fd1681-a1ae-4572-9084-52248c891464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n",
            "Embedding for the word 'king':\n",
            "[ 3.56425524e-01 -3.09262931e-01  1.64713883e+00  2.36713719e+00\n",
            "  3.12558631e-03  2.07166791e+00  6.60514474e-01  1.89080298e+00\n",
            "  2.70714343e-01  1.06550038e+00  6.26207441e-02 -4.32187825e-01\n",
            "  6.50705248e-02  3.99803567e+00 -1.24391389e+00 -7.10372865e-01\n",
            "  6.70010626e-01 -1.98204126e-02  3.31455112e+00 -2.61795402e-01\n",
            " -1.25638753e-01  1.85342634e+00 -1.34533727e+00 -1.12235701e+00\n",
            " -2.39253283e+00 -1.41270101e+00  1.59852898e+00  5.22346914e-01\n",
            " -2.51499921e-01 -3.88127542e-03  2.05755487e-01  1.02056563e+00\n",
            " -6.04657590e-01  2.67404288e-01 -2.42026914e-02  3.22725987e+00\n",
            "  6.83496237e-01 -3.28612423e+00  9.98235881e-01  3.04610491e-01\n",
            "  2.34247714e-01  6.73422873e-01 -1.04952776e+00  6.52106464e-01\n",
            " -2.08781362e+00  8.07376146e-01  2.95666289e-02 -1.16191316e+00\n",
            "  4.77713645e-01 -3.15957165e+00  1.84285879e+00  1.37641823e+00\n",
            " -2.98691154e+00  3.61585236e+00  7.00471222e-01 -1.27060390e+00\n",
            "  4.17793036e-01  1.98889291e+00  8.64774082e-03 -3.83752203e+00\n",
            "  1.74673581e+00  2.94995993e-01  3.03683662e+00  4.65668708e-01\n",
            " -2.74060816e-01 -3.48199153e+00 -1.57303953e+00 -2.20296130e-01\n",
            " -8.22832584e-01 -3.30080843e+00  9.86727893e-01 -4.82470321e-04\n",
            " -2.71437383e+00 -3.05621099e+00 -9.91903126e-01 -4.15119123e+00\n",
            "  2.05154705e+00  3.58983517e+00 -3.51610708e+00  8.11321557e-01\n",
            " -7.45022774e-01 -1.93864226e+00  9.12086904e-01  1.47431087e+00\n",
            " -1.92494428e+00  1.50794339e+00  2.63877439e+00  1.16459094e-01\n",
            " -1.16995227e+00  3.03823948e-01 -7.83934951e-01 -4.46916610e-01\n",
            " -1.35068452e+00 -1.33464360e+00  4.59600687e+00  2.04295531e-01\n",
            "  1.32199502e+00  1.30789661e+00 -4.08440018e+00 -5.60963988e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t859bLmVCe07"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}